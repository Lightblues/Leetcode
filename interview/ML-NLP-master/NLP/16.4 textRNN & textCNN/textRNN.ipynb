{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集下载\n",
    "\n",
    "使用THUCNews的一个子集进行训练与测试：[https://www.lanzous.com/i5t0lsd](https://www.lanzous.com/i5t0lsd)\n",
    "\n",
    "本次训练使用了其中的10个分类，每个分类6500条数据。类别如下：\n",
    "\n",
    "体育, 财经, 房产, 家居, 教育, 科技, 时尚, 时政, 游戏, 娱乐\n",
    "\n",
    "### cnews_loader.py为数据的预处理文件。\n",
    "\n",
    "- read_file(): 读取文件数据;\n",
    "- build_vocab(): 构建词汇表，使用字符级的表示，这一函数会将词汇表存储下来，避免每一次重复处理;\n",
    "- read_vocab(): 读取上一步存储的词汇表，转换为{词：id}表示;\n",
    "- read_category(): 将分类目录固定，转换为{类别: id}表示;\n",
    "- to_words(): 将一条由id表示的数据重新转换为文字;\n",
    "- process_file(): 将数据集从文字转换为固定长度的id序列表示;\n",
    "- batch_iter(): 为神经网络的训练准备经过shuffle的批次的数据。\n",
    "    \n",
    "### textRNN模型和可配置的参数，在rnn_model.py中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "from rnn_model import TRNNConfig, TextRNN\n",
    "from cnews_loader import read_vocab, read_category, batch_iter, process_file, build_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'cnews'\n",
    "train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "\n",
    "save_dir = 'checkpoints/textrnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')  # 最佳验证结果保存路径\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def feed_data(x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluate(sess, x_, y_):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    # 配置 Tensorboard，重新训练时，请将tensorboard文件夹删除，不然图会覆盖\n",
    "    tensorboard_dir = 'tensorboard/textrnn'\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "\n",
    "    # 配置 Saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(\"Loading training and validation data...\")\n",
    "    # 载入训练集与验证集\n",
    "    start_time = time.time()\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # 创建session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # 总批次\n",
    "    best_acc_val = 0.0  # 最佳验证集准确率\n",
    "    last_improved = 0  # 记录上一次提升批次\n",
    "    require_improvement = 1000  # 如果超过1000轮未提升，提前结束训练\n",
    "\n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n",
    "\n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # 每多少轮次输出在训练集和验证集上的性能\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # 保存最好结果\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "            \n",
    "            feed_dict[model.keep_prob] = config.dropout_keep_prob\n",
    "            session.run(model.optim, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 验证集正确率长期不提升，提前结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "        if flag:  # 同上\n",
    "            break\n",
    "\n",
    "\n",
    "def test():\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path=save_path)  # 读取保存的模型\n",
    "\n",
    "    print('Testing...')\n",
    "    loss_test, acc_test = evaluate(session, x_test, y_test)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "    batch_size = 128\n",
    "    data_len = len(x_test)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    y_test_cls = np.argmax(y_test, 1)\n",
    "    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # 保存预测结果\n",
    "    for i in range(num_batch):  # 逐批次处理\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        feed_dict = {\n",
    "            model.input_x: x_test[start_id:end_id],\n",
    "            model.keep_prob: 1.0\n",
    "        }\n",
    "        y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "    # 评估\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n",
    "\n",
    "    # 混淆矩阵\n",
    "    print(\"Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "    print(cm)\n",
    "\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 20:37:22.551977 140497609688896 module_wrapper.py:136] From /usr/local/python3/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0826 20:37:22.569656 140497609688896 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0826 20:37:22.570458 140497609688896 deprecation.py:323] From /home/python_home/WeiZhongChuang/ML/RNN/rnn_model.py:48: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0826 20:37:22.588945 140497609688896 deprecation.py:323] From /home/python_home/WeiZhongChuang/ML/RNN/rnn_model.py:65: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0826 20:37:22.589862 140497609688896 deprecation.py:323] From /home/python_home/WeiZhongChuang/ML/RNN/rnn_model.py:67: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0826 20:37:22.668138 140497609688896 deprecation.py:323] From /usr/local/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:558: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "W0826 20:37:22.677227 140497609688896 deprecation.py:506] From /usr/local/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:564: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0826 20:37:22.689308 140497609688896 deprecation.py:506] From /usr/local/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:574: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring RNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 20:37:22.789028 140497609688896 deprecation.py:323] From /home/python_home/WeiZhongChuang/ML/RNN/rnn_model.py:72: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "W0826 20:37:22.790071 140497609688896 deprecation.py:323] From /usr/local/python3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W0826 20:37:22.828924 140497609688896 deprecation.py:323] From /home/python_home/WeiZhongChuang/ML/RNN/rnn_model.py:82: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "W0826 20:37:22.848501 140497609688896 module_wrapper.py:136] From /usr/local/python3/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0826 20:37:23.638830 140497609688896 module_wrapper.py:136] From /usr/local/python3/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring TensorBoard and Saver...\n",
      "Loading training and validation data...\n",
      "Time usage: 0:00:13\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "Iter:      0, Train Loss:    2.3, Train Acc:  11.72%, Val Loss:    2.3, Val Acc:   9.28%, Time: 0:00:16 *\n",
      "Iter:    100, Train Loss:    0.9, Train Acc:  70.31%, Val Loss:    1.2, Val Acc:  61.48%, Time: 0:03:20 *\n",
      "Iter:    200, Train Loss:   0.53, Train Acc:  84.38%, Val Loss:   0.82, Val Acc:  74.44%, Time: 0:06:34 *\n",
      "Iter:    300, Train Loss:    0.4, Train Acc:  86.72%, Val Loss:   0.64, Val Acc:  82.04%, Time: 0:09:52 *\n",
      "Epoch: 2\n",
      "Iter:    400, Train Loss:   0.41, Train Acc:  89.84%, Val Loss:   0.61, Val Acc:  82.94%, Time: 0:13:10 *\n",
      "Iter:    500, Train Loss:   0.38, Train Acc:  89.84%, Val Loss:   0.51, Val Acc:  86.54%, Time: 0:16:30 *\n",
      "Iter:    600, Train Loss:   0.23, Train Acc:  92.19%, Val Loss:   0.54, Val Acc:  85.46%, Time: 0:19:51 \n",
      "Iter:    700, Train Loss:   0.11, Train Acc:  96.88%, Val Loss:    0.5, Val Acc:  86.48%, Time: 0:23:13 \n",
      "Epoch: 3\n",
      "Iter:    800, Train Loss:    0.2, Train Acc:  95.31%, Val Loss:   0.49, Val Acc:  87.02%, Time: 0:26:34 *\n",
      "Iter:    900, Train Loss:   0.19, Train Acc:  96.09%, Val Loss:   0.41, Val Acc:  89.22%, Time: 0:29:56 *\n",
      "Iter:   1000, Train Loss:    0.1, Train Acc:  96.88%, Val Loss:    0.4, Val Acc:  90.08%, Time: 0:33:18 *\n",
      "Iter:   1100, Train Loss:   0.18, Train Acc:  94.53%, Val Loss:   0.37, Val Acc:  90.50%, Time: 0:36:41 *\n",
      "Epoch: 4\n",
      "Iter:   1200, Train Loss:   0.11, Train Acc:  96.09%, Val Loss:   0.38, Val Acc:  90.24%, Time: 0:40:03 \n",
      "Iter:   1300, Train Loss:   0.13, Train Acc:  94.53%, Val Loss:   0.34, Val Acc:  91.80%, Time: 0:43:26 *\n",
      "Iter:   1400, Train Loss:   0.28, Train Acc:  93.75%, Val Loss:   0.31, Val Acc:  91.44%, Time: 0:46:48 \n",
      "Iter:   1500, Train Loss:   0.18, Train Acc:  92.97%, Val Loss:   0.31, Val Acc:  92.32%, Time: 0:50:11 *\n",
      "Epoch: 5\n",
      "Iter:   1600, Train Loss:  0.029, Train Acc: 100.00%, Val Loss:   0.47, Val Acc:  87.54%, Time: 0:53:32 \n",
      "Iter:   1700, Train Loss:  0.086, Train Acc:  96.88%, Val Loss:   0.37, Val Acc:  90.70%, Time: 0:56:55 \n",
      "Iter:   1800, Train Loss:   0.21, Train Acc:  93.75%, Val Loss:   0.32, Val Acc:  91.16%, Time: 1:00:18 \n",
      "Iter:   1900, Train Loss:   0.16, Train Acc:  95.31%, Val Loss:   0.31, Val Acc:  91.26%, Time: 1:03:41 \n",
      "Epoch: 6\n",
      "Iter:   2000, Train Loss:  0.059, Train Acc:  98.44%, Val Loss:   0.35, Val Acc:  91.70%, Time: 1:07:03 \n",
      "Iter:   2100, Train Loss:  0.085, Train Acc:  98.44%, Val Loss:   0.34, Val Acc:  90.58%, Time: 1:10:26 \n",
      "Iter:   2200, Train Loss:  0.067, Train Acc:  98.44%, Val Loss:   0.31, Val Acc:  91.86%, Time: 1:13:49 \n",
      "Iter:   2300, Train Loss:  0.035, Train Acc:  99.22%, Val Loss:   0.32, Val Acc:  91.66%, Time: 1:17:11 \n",
      "Epoch: 7\n",
      "Iter:   2400, Train Loss:  0.026, Train Acc: 100.00%, Val Loss:   0.39, Val Acc:  90.64%, Time: 1:20:34 \n",
      "Iter:   2500, Train Loss:  0.027, Train Acc:  98.44%, Val Loss:   0.44, Val Acc:  89.42%, Time: 1:23:57 \n",
      "No optimization for a long time, auto-stopping...\n"
     ]
    }
   ],
   "source": [
    "type_ = 'train'\n",
    "\n",
    "print('Configuring RNN model...')\n",
    "config = TRNNConfig()\n",
    "if not os.path.exists(vocab_dir):  # 如果不存在词汇表，重建\n",
    "    build_vocab(train_dir, vocab_dir, config.vocab_size)\n",
    "categories, cat_to_id = read_category()\n",
    "words, word_to_id = read_vocab(vocab_dir)\n",
    "config.vocab_size = len(words)\n",
    "model = TextRNN(config)\n",
    "\n",
    "if type_ == 'train':\n",
    "    train()\n",
    "else:\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Testing...\n",
      "Test Loss:   0.19, Test Acc:  94.51%\n",
      "Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          体育       0.99      0.99      0.99      1000\n",
      "          财经       0.92      0.99      0.95      1000\n",
      "          房产       1.00      1.00      1.00      1000\n",
      "          家居       0.98      0.83      0.90      1000\n",
      "          教育       0.88      0.91      0.90      1000\n",
      "          科技       0.93      0.97      0.95      1000\n",
      "          时尚       0.90      0.97      0.93      1000\n",
      "          时政       0.96      0.86      0.91      1000\n",
      "          游戏       0.96      0.96      0.96      1000\n",
      "          娱乐       0.95      0.98      0.96      1000\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.94     10000\n",
      "weighted avg       0.95      0.95      0.94     10000\n",
      "\n",
      "Confusion Matrix...\n",
      "[[987   1   0   0   8   2   0   0   2   0]\n",
      " [  1 987   0   0   2   0   0   8   0   2]\n",
      " [  0   0 996   2   0   0   0   0   1   1]\n",
      " [  4  26   0 831  27  25  65   4   4  14]\n",
      " [  5   8   0   5 914  16  13  23  10   6]\n",
      " [  1   1   0   4   7 967   4   1  11   4]\n",
      " [  1   0   0   5   8   1 971   1   3  10]\n",
      " [  0  51   0   2  60  19   1 863   1   3]\n",
      " [  0   0   0   0   9   5  20   0 957   9]\n",
      " [  2   3   0   2   2   1   7   1   4 978]]\n",
      "Time usage: 0:01:05\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
